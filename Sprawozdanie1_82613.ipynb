{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5aqzMW3J-Y7"
      },
      "source": [
        "# **Sprawozdanie 1 - Big data: przetwarzanie i analiza dużych zbiorów danych**\n",
        "### Imię i nazwisko: Mateusz Boguszewski\n",
        "### Data: 11.05.2024\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Zostalo zastosowane podejscie wybrania waznych cech przed podziałem danych na treningowe oraz testowe. Powodem byl problem z zamiana danych tekstowych na danych testowych poniewaz zawieraly one dodatkowe pozycje, ktore nie trafily do danych treningowych. Nie mialem pomyslu jak to rozwiazac wiec zastosowalem takie podejscie."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTmLoXebJ-ql"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## **I. Listing, kod modułu 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dane byly trudne do polaczenia poniewaz dane kliniczne zawieraly kolumne z ID pacjenta a dane genetyczne mialy x kolumn w ktorych nazwach bylo ID pacjenta, dane mialy tez kilka wierszy co przy standardowym podejsciu do polaczenia ich dalo olbrzymia ramke danych z kilkukrotnie powtarzajcym sie pacjentem. Po polaczeniu danych uzupelniam brakujace wartosci, zamieniam wartosci tekstowe na numeryczne, normalizuje dane, pozbywam sie cech silnie skorelowanych ze zmienna decyzyjna a na koniec za pomoca RandomForest wybieram 100 najwazniejszych cech i zostawiam w danych tylko je."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxoiVgvfKudu"
      },
      "outputs": [],
      "source": [
        "def modul1(kliniczne, genetyczne):\n",
        "\n",
        "    clinical_data = pd.read_csv(kliniczne, comment='#', sep='\\t')\n",
        "    genetic_data = pd.read_csv(genetyczne, sep='\\t')\n",
        "\n",
        "    # Transpozycja danych\n",
        "    genetic_data_transposed = genetic_data.transpose()\n",
        "    new_columns = [f\"{gene}_{id}\" for gene, id in zip(genetic_data_transposed.iloc[0], genetic_data_transposed.iloc[1])]\n",
        "    genetic_data_transposed.columns = new_columns\n",
        "\n",
        "    # Usun te dwa pierwsze wiersze, są danymi kolumn\n",
        "    genetic_data_transposed = genetic_data_transposed.drop(genetic_data_transposed.index[:2])\n",
        "\n",
        "    # Usuniecie -1 z id pacjenta\n",
        "    genetic_data_transposed.index = genetic_data_transposed.index.str.replace('-1', '')\n",
        "    # print(genetic_data_transposed.head(50))\n",
        "\n",
        "    # Polaczenie danych po ID pacjentów\n",
        "    merged_data = pd.merge(clinical_data, genetic_data_transposed, left_on='PATIENT_ID', right_index=True)\n",
        "\n",
        "    data = merged_data\n",
        "    data['VITAL_STATUS'] = data['VITAL_STATUS'].map({'Alive': 1, 'Dead': 0})\n",
        "    data = data.drop(['OTHER_PATIENT_ID'], axis=1)\n",
        "    data.dropna(axis=1, how='all', inplace=True)\n",
        "\n",
        "    # Imputacja brakujących danych\n",
        "\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    numeric_columns = data.select_dtypes(include=[np.number]).columns\n",
        "    data[numeric_columns] = imputer.fit_transform(data[numeric_columns])\n",
        "\n",
        "    categorical_columns = data.select_dtypes(include=[object]).columns\n",
        "\n",
        "    # Imputacja najczęściej występującą wartością dla kolumn kategorycznych\n",
        "    categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
        "    data[categorical_columns] = categorical_imputer.fit_transform(data[categorical_columns])\n",
        "\n",
        "    label_encoders = {}\n",
        "    for column in data.select_dtypes(include=['object']).columns:\n",
        "        le = LabelEncoder()\n",
        "        data[column] = le.fit_transform(data[column].astype(str))\n",
        "        label_encoders[column] = le\n",
        "\n",
        "    # Wybieramy kolumny do normalizacji (wszystkie poza 'VITAL_STATUS')\n",
        "    features = data.columns\n",
        "\n",
        "    # Tworzymy instancję MinMaxScaler\n",
        "    scaler = MinMaxScaler()\n",
        "\n",
        "    # Normalizujemy dane\n",
        "    data[features] = scaler.fit_transform(data[features])\n",
        "\n",
        "    correlation_matrix = data.corr()\n",
        "\n",
        "    # Wyciągnięcie korelacji względem 'VITAL_STATUS'\n",
        "    correlation_with_target = correlation_matrix['VITAL_STATUS'].abs()\n",
        "\n",
        "    # Wybór kolumn do usunięcia (próg korelacji ustawiony na 0.7)\n",
        "    high_correlation_features = correlation_with_target[correlation_with_target > 0.7].index.tolist()\n",
        "    high_correlation_features.remove('VITAL_STATUS')  # Usunięcie kolumny 'VITAL_STATUS' z listy\n",
        "\n",
        "    # Usunięcie wybranych kolumn z DataFrame\n",
        "    data = data.drop(columns=high_correlation_features)\n",
        "    features = data.columns.difference(['VITAL_STATUS'])\n",
        "\n",
        "    # Ponowna normalizacja danych\n",
        "    data[features] = scaler.fit_transform(data[features])\n",
        "\n",
        "    # Trenowanie lasu losowego dla selekcji cech\n",
        "    rf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
        "    rf.fit(data.drop('VITAL_STATUS', axis=1), data['VITAL_STATUS'])\n",
        "\n",
        "    # Wybór 100 najważniejszych cech\n",
        "    important_features = pd.Series(rf.feature_importances_, index=data.columns[1:]).nlargest(100).index\n",
        "    data = data[important_features.tolist()]\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YZCSTuAKvd8"
      },
      "source": [
        "## **II. Listing, kod modułu 2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkIMC3bWKxA9"
      },
      "outputs": [],
      "source": [
        "def modul2(data):\n",
        "    # Podział danych na treningowe i testowe (przyjmujemy 70% trening, 30% test)\n",
        "    train_data, test_data = train_test_split(data, test_size=0.3, random_state=0)\n",
        "\n",
        "    # Trenowanie modelu SVM\n",
        "    svm_model = SVC(kernel='rbf')\n",
        "    svm_model.fit(train_data.drop('VITAL_STATUS', axis=1), train_data['VITAL_STATUS'])\n",
        "\n",
        "    predictions = svm_model.predict(test_data.drop('VITAL_STATUS', axis=1))\n",
        "    auc_score = roc_auc_score(test_data['VITAL_STATUS'], predictions)\n",
        "    accuracy = accuracy_score(test_data['VITAL_STATUS'], predictions)\n",
        "    cm = confusion_matrix(test_data['VITAL_STATUS'], predictions)\n",
        "    print(cm)\n",
        "    print(f\"AUC Score: {auc_score}\")\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    modul3(svm_model, predictions, test_data)\n",
        "\n",
        "def data_abc(data, znacznik):\n",
        "\n",
        "    predictors = data.columns.difference(['VITAL_STATUS'])\n",
        "\n",
        "    if znacznik == 'A' or znacznik == 'a':\n",
        "        DATA_A = data.copy()\n",
        "        # Odejmowanie losowych 2% wartości początkowej każdego predyktora\n",
        "        noise = np.random.rand(*DATA_A[predictors].shape) * 0.02\n",
        "        DATA_A[predictors] -= DATA_A[predictors] * noise\n",
        "        return modul2(DATA_A)\n",
        "\n",
        "    elif znacznik == 'B' or znacznik == 'b':\n",
        "        DATA_B = data.copy()\n",
        "        # Dodawanie losowych 3% wartości początkowej każdego predyktora\n",
        "        noise = np.random.rand(*DATA_B[predictors].shape) * 0.03\n",
        "        DATA_B[predictors] += DATA_B[predictors] * noise\n",
        "\n",
        "        # Usuwanie losowo 5% wierszy\n",
        "        rows_to_drop = np.random.choice(DATA_B.index, size=int(len(DATA_B) * 0.05), replace=False)\n",
        "        DATA_B = DATA_B.drop(rows_to_drop)\n",
        "        return modul2(DATA_B)\n",
        "\n",
        "    elif znacznik == 'C' or znacznik == 'c':\n",
        "        DATA_C = data.copy()\n",
        "        # Dodawanie losowych 3% wartości początkowej każdego predyktora\n",
        "        noise = np.random.rand(*DATA_C[predictors].shape) * 0.03\n",
        "        DATA_C[predictors] += DATA_C[predictors] * noise\n",
        "\n",
        "        # Usuwanie losowo 20% kolumn predyktorów\n",
        "        columns_to_drop = np.random.choice(predictors, size=int(len(predictors) * 0.2), replace=False)\n",
        "        DATA_C = DATA_C.drop(columns=columns_to_drop)\n",
        "        return modul2(DATA_C)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QK39jRyFKyh3"
      },
      "source": [
        "## **III. Listing, kod modułu 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Wyniki zostaly zwizualizowane przy pomocy macierzy pomylek oraz krzywej ROC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJ12TalLK0B8"
      },
      "outputs": [],
      "source": [
        "\n",
        "def modul3(svm_model, predictions, test_data):\n",
        "    # Macierz pomyłek\n",
        "    cm = confusion_matrix(test_data['VITAL_STATUS'], predictions)\n",
        "\n",
        "    # Wizualizacja macierzy pomyłek\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues', xticklabels=['Dead', 'Alive'], yticklabels=['Dead', 'Alive'])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "    # Obliczanie wartości dla krzywej ROC\n",
        "    fpr, tpr, thresholds = roc_curve(test_data['VITAL_STATUS'],\n",
        "                                     svm_model.decision_function(test_data.drop('VITAL_STATUS', axis=1)))\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Rysowanie krzywej ROC\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78s_1y-hK07d"
      },
      "source": [
        "## **IV. Listing, kod modułu 4**\n",
        "\n",
        "### ***Opis:*** Poniżej przedstawiono etapy przykładowego procesu przetwarzania surowych danych, po połączeniu ramek danych zostały usunięte puste kolumny, w puste miejsca zostały dodane dane w zależności od typu danych w kolumnie: w numeryczne średnią wartość z danej kolumny i w tekstowych najczęściej występująca wartość. Dane zostały przekonwertowane do wartości numerycznych, a następnie wyskalowane. Następnie wykonałem badanie zależności cech do naszej kolumny decyzyjnej, te, które były wysoko zależne zostały usunięte z ramki. Ostatnim krokiem było stworzenie modelu lasów losowych do detekcji najważniejszych cech. Wybrałem 100 takich i zostawiłem tylko je w ramce. Po takim przekształceniu danych można je teraz podzielić na treningowe i testowe i śmiało budować model np. SVM do klasyfikacji osób chorych."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSl-Fci-OS--"
      },
      "source": [
        "### **Import bibliotek**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHGMblXrOC5d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, roc_curve, auc\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVk8dKhDJ-7N"
      },
      "source": [
        "###  **Odczyt danych**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXG4nzUBL4ql"
      },
      "outputs": [],
      "source": [
        "clinical_data = pd.read_csv('data_clinical_patient.txt', comment='#', sep='\\t')\n",
        "genetic_data = pd.read_csv('data_genetic.txt', sep='\\t')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0w-zdyT1MQfN"
      },
      "source": [
        "###  **Integracja danych**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIRsJqFaN5zm"
      },
      "outputs": [],
      "source": [
        "# Transpozycja danych\n",
        "genetic_data_transposed = genetic_data.transpose()\n",
        "new_columns = [f\"{gene}_{id}\" for gene, id in zip(genetic_data_transposed.iloc[0], genetic_data_transposed.iloc[1])]\n",
        "genetic_data_transposed.columns = new_columns\n",
        "\n",
        "# Usun te dwa pierwsze wiersze, są danymi kolumn\n",
        "genetic_data_transposed = genetic_data_transposed.drop(genetic_data_transposed.index[:2])\n",
        "\n",
        "# Usuniecie -1 z id pacjenta\n",
        "genetic_data_transposed.index = genetic_data_transposed.index.str.replace('-1', '')\n",
        "#print(genetic_data_transposed.head(50))\n",
        "\n",
        "# Polaczenie danych po ID pacjentów\n",
        "merged_data = pd.merge(clinical_data, genetic_data_transposed, left_on='PATIENT_ID', right_index=True)\n",
        "\n",
        "data = merged_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgTkKuION6EE"
      },
      "source": [
        "###  **Czyszczenie danych**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlm-P15HMSCf"
      },
      "outputs": [],
      "source": [
        "data = data.drop(['OTHER_PATIENT_ID'], axis=1)\n",
        "label_encoders = {}\n",
        "for column in data.select_dtypes(include=['object']).columns:\n",
        "    le = LabelEncoder()\n",
        "    data[column] = le.fit_transform(data[column].astype(str))\n",
        "    label_encoders[column] = le"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFuZFhL7MUvN"
      },
      "source": [
        "###  **Imputacja danych**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCD4TN35MWQ8"
      },
      "outputs": [],
      "source": [
        "# Imputacja brakujących danych\n",
        "\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "numeric_columns = data.select_dtypes(include=[np.number]).columns\n",
        "data[numeric_columns] = imputer.fit_transform(data[numeric_columns])\n",
        "\n",
        "categorical_columns = data.select_dtypes(include=[object]).columns\n",
        "\n",
        "# Imputacja najczęściej występującą wartością dla kolumn kategorycznych\n",
        "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
        "data[categorical_columns] = categorical_imputer.fit_transform(data[categorical_columns])\n",
        "data.dropna(axis=1, how='all', inplace=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwbEAKFPMWgf"
      },
      "source": [
        "###  **Skalowanie**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wybieramy kolumny do normalizacji (wszystkie poza 'VITAL_STATUS')\n",
        "features = data.columns\n",
        "\n",
        "# Tworzymy instancję MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Normalizujemy dane\n",
        "data[features] = scaler.fit_transform(data[features])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  **Badanie korelacji**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "correlation_matrix = data.corr()\n",
        "\n",
        "# Wyciągnięcie korelacji względem 'VITAL_STATUS'\n",
        "correlation_with_target = correlation_matrix['VITAL_STATUS'].abs()\n",
        "\n",
        "# Wybór kolumn do usunięcia (próg korelacji ustawiony na 0.7)\n",
        "high_correlation_features = correlation_with_target[correlation_with_target > 0.7].index.tolist()\n",
        "high_correlation_features.remove('VITAL_STATUS')  # Usunięcie kolumny 'VITAL_STATUS' z listy\n",
        "\n",
        "# Usunięcie wybranych kolumn z DataFrame\n",
        "data = data.drop(columns=high_correlation_features)\n",
        "features = data.columns.difference(['VITAL_STATUS'])\n",
        "\n",
        "#Ponowna normalizacja danych\n",
        "data[features] = scaler.fit_transform(data[features])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  **Wybór 100 najlepszych cech**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Trenowanie lasu losowego dla selekcji cech\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
        "rf.fit(data.drop('VITAL_STATUS', axis=1), data['VITAL_STATUS'])\n",
        "\n",
        "# Wybór 100 najważniejszych cech\n",
        "important_features = pd.Series(rf.feature_importances_, index=data.columns[1:]).nlargest(100).index\n",
        "data = data[important_features.tolist()]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
